{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Example\n",
    "\n",
    "Build a 2-hidden layers fully connected neural network (a.k.a multilayer perceptron) with TensorFlow v2.\n",
    "\n",
    "This example is using a low-level approach to better understand all mechanics behind building neural networks and the training process.\n",
    "\n",
    "- Author: Miguel Tom√°s\n",
    "- Project: https://github.com/aeonSolutions/TensorFlow-Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Overview\n",
    "\n",
    "<img src=\"http://cs231n.github.io/assets/nn1/neural_net2.jpeg\" alt=\"nn\" style=\"width: 400px;\"/>\n",
    "\n",
    "This example is using a file csv dataset.  \n",
    "\n",
    "In this example, each dataset will be converted to float32, normalized to [0, 1] and flattened to a 1-D array of \"num_features\" features \n",
    "\n",
    "More info: https://github.com/aeonSolutions/TensorFlow-Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, layers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Visualize predictions.\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task type to perform\n",
    "taskRunning=\"regression\" # can be: classification / regression\n",
    "\n",
    "# parameters initialization.\n",
    "num_classes = 1 # total classes : number of output results wanted\n",
    "num_features = 0 # data features : number of input variables on the dataset. a value of 0 loads from the dataset bellow\n",
    "\n",
    "# Training parameters.\n",
    "learning_rate = 1\n",
    "training_steps = 10000\n",
    "batch_size = 24\n",
    "display_step = 100\n",
    "\n",
    "#normalization of data\n",
    "normalizeDataValues=True\n",
    "normalizationType= \"mean\" # accepts: max, mean\n",
    "normalizationTypeBinary=True\n",
    "\n",
    "# Network parameters.\n",
    "n_hidden_1 = 512 # 1st layer number of neurons.\n",
    "n_hidden_2 = 512 # 2nd layer number of neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions Data.\n",
    "df_predict_ds=pd.read_csv('./week3_exam_dataset_test.csv')\n",
    "\n",
    "data_predict_x = np.float32(df_predict_ds.values)\n",
    "\n",
    "# Training Data.\n",
    "df_tr=pd.read_csv('./week3_exam_dataset_train.csv')\n",
    "\n",
    "df_tr_raw_y= df_tr['y']\n",
    "\n",
    "df_tr_raw_values_y = df_tr_raw_y.values\n",
    "data_tr_y = np.float32(df_tr_raw_values_y)\n",
    "\n",
    "df_tr_raw_x= df_tr.drop('y',1)\n",
    "if num_features==0:\n",
    "    num_features= df_tr_raw_x.shape[1]\n",
    "else:\n",
    "    rs=\"\"\n",
    "    #TODO: fill possible empty values on the datasets\n",
    "df_tr_raw_values_x = df_tr_raw_x.values\n",
    "data_tr_x = np.float32(df_tr_raw_values_x)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_tr_raw_x, df_tr_raw_y, test_size=0.33, random_state=42)\n",
    "\n",
    "# Convert to float32.\n",
    "x_train, x_test = np.array(x_train, np.float32), np.array(x_test, np.float32)\n",
    "# Convert to float32.\n",
    "y_train, y_test = np.array(y_train, np.float32), np.array(y_test, np.float32)\n",
    "\n",
    "# Normalize data values to [0, 1] interval.\n",
    "if normalizeDataValues:\n",
    "    if normalizationType==\"max\":\n",
    "        maxVal_train=np.amax(x_train, axis=0)\n",
    "        maxVal_test=np.amax(x_test, axis=0)\n",
    "        maxVal_pred=np.amax(data_predict_x, axis=0)\n",
    "        maxVal=max(np.amax(maxVal_train, axis=0),np.amax(maxVal_test, axis=0),np.amax(maxVal_pred, axis=0))\n",
    "        if normalizationTypeBinary:\n",
    "            x_train, x_test,data_predict_x = np.where((x_train / maxVal_train)>=0.5,1,0), np.where((x_test / maxVal_test)>=0.5,1,0), np.where((data_predict_x / maxVal_pred)>=0.5,1,0)\n",
    "        else:\n",
    "            x_train, x_test, data_predict_x = x_train / maxVal_train, x_test / maxVal_test, data_predict_x / maxVal_pred\n",
    "    else:\n",
    "        meanVal_test=np.mean(x_test, axis=0)\n",
    "        meanVal_train=np.mean(x_train, axis=0)\n",
    "        meanVal_pred=np.mean(data_predict_x, axis=0)\n",
    "        meanVal=max(np.amax(meanVal_train, axis=0),np.amax(meanVal_test, axis=0),np.amax(meanVal_pred, axis=0))\n",
    "        \n",
    "        if normalizationTypeBinary:\n",
    "            x_train, x_test,data_predict_x = np.where((x_train / meanVal_train)>=0.5,1,0), np.where((x_test / meanVal_test)>=0.5,1,0), np.where((data_predict_x / meanVal_pred)>=0.5,1,0)\n",
    "        else:\n",
    "            x_train, x_test, data_predict_x = x_train / meanVal_train, x_test / meanVal_test, data_predict_x / meanVal_pred\n",
    "\n",
    "# Use tf.data API to shuffle and batch data.\n",
    "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_data = train_data.repeat().shuffle(5000).batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF Model.\n",
    "class NeuralNet(Model):\n",
    "    # Set layers.\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        # First fully-connected hidden layer.\n",
    "        self.fc1 = layers.Dense(n_hidden_1, activation=tf.nn.relu)\n",
    "        # First fully-connected hidden layer.\n",
    "        self.fc2 = layers.Dense(n_hidden_2, activation=tf.nn.relu)\n",
    "        # Second fully-connecter hidden layer.\n",
    "        self.out = layers.Dense(num_classes)\n",
    "\n",
    "    # Set forward pass.\n",
    "    def call(self, x, is_training=False):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.out(x)\n",
    "        if not is_training:\n",
    "            # tf cross entropy expect logits without softmax, so only\n",
    "            # apply softmax when not training.\n",
    "            x = tf.nn.softmax(x)\n",
    "        return x\n",
    "\n",
    "# Build neural network model.\n",
    "neural_net = NeuralNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Entropy Loss.\n",
    "# Note that this will apply 'softmax' to the logits.\n",
    "def cross_entropy_loss(x, y):\n",
    "    # Convert labels to int 64 for tf cross-entropy function.\n",
    "    y = tf.cast(y, tf.int32)\n",
    "    if (taskRunning==\"classification\"):\n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=x)\n",
    "    else:    \n",
    "        loss=tf.keras.losses.binary_crossentropy(y, x)\n",
    "    # Apply softmax to logits and compute cross-entropy.\n",
    "    # Average loss across the batch.\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "# Accuracy metric.\n",
    "def accuracy(y_pred, y_true):\n",
    "    # Predicted class is the index of highest score in prediction vector (i.e. argmax).\n",
    "    correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.cast(y_true, tf.int64))\n",
    "    return tf.reduce_mean(tf.cast(correct_prediction, tf.float32), axis=-1)\n",
    "\n",
    "# Accuracy metric.\n",
    "def accuracyAvg(y_pred):\n",
    "    print(y_pred.numpy().shape)\n",
    "    \n",
    "    #convert to 1D array\n",
    "    y_pred_1d_array= y_pred.ravel()\n",
    "    \n",
    "    accCalc= np.full(y_pred_1d_array.shape, 0)\n",
    "    delta=np.amax(real_y_1d_array)-np.amin(real_y_1d_array)\n",
    "    \n",
    "    for i in range(len(y_pred_1d_array)):\n",
    "        accCalc[i]= abs(delta-y_pred_1d_array[i] - real_y_1d_array[i])\n",
    "    \n",
    "    return accCalc\n",
    "\n",
    "# Stochastic gradient descent optimizer.\n",
    "optimizer = tf.optimizers.SGD(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization process. \n",
    "def run_optimization(x, y):\n",
    "    # Wrap computation inside a GradientTape for automatic differentiation.\n",
    "    with tf.GradientTape() as g:\n",
    "        # Forward pass.\n",
    "        pred = neural_net(x, is_training=True)\n",
    "        # Compute loss.\n",
    "        loss = cross_entropy_loss(pred, y)\n",
    "        \n",
    "    # Variables to update, i.e. trainable variables.\n",
    "    trainable_variables = neural_net.trainable_variables\n",
    "\n",
    "    # Compute gradients.\n",
    "    gradients = g.gradient(loss, trainable_variables)\n",
    "    \n",
    "    # Update W and b following gradients.\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "def live_plot(steps, accuracy, figsize=(7,5), title=''):\n",
    "    clear_output(wait=True)\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.xlim(0, training_steps)\n",
    "    plt.ylim(0, 100)\n",
    "    steps= [float(i) for i in steps]\n",
    "    accuracy= [float(i) for i in accuracy]\n",
    "    \n",
    "    m=0\n",
    "    if len(steps) > 1:\n",
    "        plt.scatter(steps,accuracy, label='accuracy', color='k') \n",
    "        m, b = np.polyfit(steps, accuracy, 1)\n",
    "        plt.plot(steps, [x * m for x in steps] + b)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy %')\n",
    "    #plt.legend(loc='center left') # the plot evolves to the right\n",
    "    plt.show();\n",
    "    return m\n",
    "\n",
    "def ETC(start, steps):\n",
    "    time_elapsed = datetime.now() - start\n",
    "    eta= (training_steps-steps) / display_step * time_elapsed\n",
    "    #avgString = str(avg).split(\".\")[0]\n",
    "    \n",
    "    hours= int(eta.seconds/3600)\n",
    "    minutes= int((eta.seconds/60)-hours*60)\n",
    "    seconds = int(eta.seconds - minutes*60 -hours*3600)\n",
    "    return \"%sh, %s min and %s sec\" % (hours, minutes, seconds)\n",
    "\n",
    "def elapsedTime(elapsed):\n",
    "    hours= int(elapsed.seconds/3600)\n",
    "    minutes= int((elapsed.seconds/60)-hours*60)\n",
    "    seconds = int(elapsed.seconds - minutes*60 -hours*3600)\n",
    "    return \"%sh, %s min and %s sec\" % (hours, minutes, seconds)\n",
    "\n",
    "def progress(percent=0, width=30):\n",
    "    left = width * percent // 100\n",
    "    right = width - left\n",
    "    print('\\r[', '#' * left, ' ' * right, ']',\n",
    "          f' {percent:.0f}%\\n',\n",
    "          sep='', end='', flush=True)\n",
    "    \n",
    "def measureSkewness(series):    \n",
    "    result=\"\"\n",
    "    \n",
    "    if (series.skew() > 1 or series.skew() < -1):\n",
    "        result=\"Highly skewed distribution\"\n",
    "    elif((0.5 < series.skew() < 1) or (-1 < series.skew() < -0.5)):\n",
    "        result=\"Moderately skewed distribution\"\n",
    "    elif(-0.5 < series.skew() < 0.5):\n",
    "        result=\"Approximately symmetric distribution\"\n",
    "    result = result +\" ( \" +str(round(series.skew(),2))+\" )\"\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running...\n",
      "[###############               ] 52%\n",
      "\n",
      "================= Iteration Stats ================\n",
      "                   step: 5200 of 10000\n",
      "                   loss: 1.285412\n",
      "\n",
      "               accuracy: 91.67 %\n",
      "                    AVG: 74.92 %\n",
      "                   Best: 91.67 %\n",
      "\n",
      "            Trend slope: -0.000\n",
      "         MAD Dispersion: nan\n",
      "               Skewness: Approximately symmetric distribution ( -0.29 )\n",
      "\n",
      "================= Time           ================\n",
      "                Elapsed: 0h, 1 min and 10 sec\n",
      "                    ETC: 0h, 1 min and 22 sec\n",
      "\n",
      "================= Network Setup  ================\n",
      "      number of classes: 1\n",
      "     number of features: 14\n",
      "          learning rate: 1\n",
      "         training steps: 10000\n",
      "             batch size: 24\n",
      "1st layer n. of neurons: 512\n",
      "2st layer n. of neurons: 512\n",
      "        Normalized data: True, type: mean, Discrete Binary 0/1\n"
     ]
    }
   ],
   "source": [
    "# Run training for the given number of steps.\n",
    "avgCounter=0\n",
    "avg=0.0\n",
    "\n",
    "steps=[]\n",
    "accuracyValue=[]\n",
    "\n",
    "start_time = datetime.now()\n",
    "totalStartTime=start_time\n",
    "    \n",
    "live_plot([0], [0])\n",
    "print(\"analysis started. Waiting for preliminary data. One moment please...\")\n",
    "progress(0) \n",
    "\n",
    "for step, (batch_x, batch_y) in enumerate(train_data.take(training_steps), 1):\n",
    "    # Run the optimization to update W and b values.\n",
    "    run_optimization(batch_x, batch_y)\n",
    "        \n",
    "    if step % display_step == 0:\n",
    "        pred = neural_net(batch_x, is_training=True)\n",
    "        loss = cross_entropy_loss(pred, batch_y)\n",
    "        acc = accuracy(pred, batch_y)\n",
    "        avgCounter+=1\n",
    "        avg+=acc*100\n",
    "        \n",
    "        steps.append(step)\n",
    "        accuracyValue.append(acc*100)\n",
    "        trendSlope= live_plot(steps, accuracyValue)\n",
    "        totalTime =datetime.now()- totalStartTime\n",
    "        \n",
    "        stats=pd.Series(accuracyValue)\n",
    "        \n",
    "        if int(step/training_steps*100)<100:\n",
    "            print(\"Running...\")\n",
    "        progress(int(step/training_steps*100)) \n",
    "        print(\"\")\n",
    "        print(\"================= Iteration Stats ================\")\n",
    "        print(\"                   step: %i of %i\" % (step,training_steps))\n",
    "        print(\"                   loss: %f\" % loss)\n",
    "        print(\"\")\n",
    "        print(\"               accuracy: %.2f %%\" %  (acc*100))\n",
    "        print(\"                    AVG: %.2f %%\" % (avg/avgCounter))\n",
    "        print(\"                   Best: %.2f %%\" % (np.max(accuracyValue)))\n",
    "        print(\"\")\n",
    "        print(\"            Trend slope: %.3f\" % (trendSlope))\n",
    "        print(\"         MAD Dispersion: \" + str(stats.mad()))\n",
    "        print(\"               Skewness: \" + measureSkewness(stats))\n",
    "        print(\"\")\n",
    "        print(\"================= Time           ================\")\n",
    "        print(\"                Elapsed: \" + elapsedTime(totalTime))\n",
    "        print(\"                    ETC: \" + ETC(start_time,step))\n",
    "        print(\"\")\n",
    "        print(\"================= Network Setup  ================\")\n",
    "        print(\"      number of classes: \"+ str(num_classes))\n",
    "        print(\"     number of features: \"+ str(num_features)) \n",
    "\n",
    "        print(\"          learning rate: \"+ str(learning_rate))\n",
    "        print(\"         training steps: \"+ str(training_steps))\n",
    "        print(\"             batch size: \"+ str(batch_size))\n",
    "\n",
    "        print(\"1st layer n. of neurons: \"+ str(n_hidden_1 ))\n",
    "        print(\"2st layer n. of neurons: \"+ str(n_hidden_2))\n",
    "        print(\"        Normalized data: \" + (\"True\" if normalizeDataValues else \"False\") +\", type: \" + normalizationType +\", \" +(\"Discrete Binary 0/1\" if normalizationType else \"Continuous range [0,1]\"))\n",
    "\n",
    "        start_time = datetime.now()\n",
    "print(\"\")\n",
    "print(\"Trainning Analysis Finished.\")        \n",
    "print(\"Start testing test dataset...\")\n",
    "run_test = neural_net(x_test, is_training=False)\n",
    "print(\"Accuracy of highest score in prediction dataset\")\n",
    "print(\"         Test Accuracy: %.2f %%\" % (tf.math.round(100*accuracy(run_test, y_test))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_predict_x.shape)\n",
    "\n",
    "run_prediction = neural_net(data_predict_x, is_training=False)\n",
    "print(run_prediction.shape)\n",
    "\n",
    "print(np.round(run_prediction.numpy(),0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " Author: Miguel Tom√°s \n",
    "\n",
    " License: Creative Commons \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
